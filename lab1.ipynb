{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9a3cff-4fa2-4cd9-a8aa-05e24fc14c68",
   "metadata": {},
   "source": [
    "## Устанавливаем Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "51f73fd1-8980-46bd-b697-38620beb5616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\k.bazhenov\\.cache\\kagglehub\\datasets\\dmitryyemelyanov\\chinese-traffic-signs\\versions\\2\n",
      "Path to images: C:\\Users\\k.bazhenov\\.cache\\kagglehub\\datasets\\dmitryyemelyanov\\chinese-traffic-signs\\versions\\2\\images\n",
      "Количество знаков с совпадающими категориями:\n",
      "2710 2710\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"dmitryyemelyanov/chinese-traffic-signs\")\n",
    "images_path = os.path.join(path,'images')\n",
    "annotations_csv_path = os.path.join(path,'annotations.csv')\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(\"Path to images:\", images_path)\n",
    "\n",
    "sign_categories = [3,4,5,7,11,16,17,26,30,35,43,55]\n",
    "df = pd.read_csv(annotations_csv_path)\n",
    "\n",
    "# Определяем форму изображения для наших картинок, так как иначе модель не сможет тренироваться\n",
    "image_size = (32, 32)\n",
    "\n",
    "# Проверяем, какие строки имеют категории из sign_categories\n",
    "sign_images = []\n",
    "categories = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    file_name = row['file_name']\n",
    "    category = row['category']\n",
    "    x1, x2 = row['x1'], row['x2']\n",
    "    y1, y2 = row['y1'], row['y2']\n",
    "    if category in sign_categories:\n",
    "        sign_file = os.path.join(images_path, file_name)\n",
    "        image = Image.open(sign_file)\n",
    "\n",
    "        # Обрезаем изображение\n",
    "        cropped_image_array = np.array(image)[y1:y2, x1:x2]\n",
    "\n",
    "        # Изменяем размер обрезанного изображения\n",
    "\n",
    "        resized_image = Image.fromarray(cropped_image_array).resize(image_size)\n",
    "        \n",
    "        sign_images.append(resized_image)\n",
    "        categories.append(sign_categories.index(category))\n",
    "categories = to_categorical(categories)\n",
    "\n",
    "print(\"Количество знаков с совпадающими категориями:\")\n",
    "print(len(sign_images), len(categories))\n",
    "\n",
    "# Конвертируем в необходимый для тренировки вид данных\n",
    "sign_images = np.array(sign_images)\n",
    "sign_images = sign_images / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2ac69ec-effc-45e0-beed-82261f30f6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - acc: 0.4160 - loss: 1.7767 - val_acc: 0.8727 - val_loss: 0.9373\n",
      "Epoch 2/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.8905 - loss: 0.4838 - val_acc: 0.9336 - val_loss: 0.3530\n",
      "Epoch 3/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9583 - loss: 0.1802 - val_acc: 0.9705 - val_loss: 0.2137\n",
      "Epoch 4/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9594 - loss: 0.1572 - val_acc: 0.9446 - val_loss: 0.2181\n",
      "Epoch 5/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9692 - loss: 0.1256 - val_acc: 0.9779 - val_loss: 0.1162\n",
      "Epoch 6/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9842 - loss: 0.0672 - val_acc: 0.9889 - val_loss: 0.0711\n",
      "Epoch 7/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9820 - loss: 0.0799 - val_acc: 0.9926 - val_loss: 0.1238\n",
      "Epoch 8/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9841 - loss: 0.0740 - val_acc: 0.9779 - val_loss: 0.0954\n",
      "Epoch 9/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9864 - loss: 0.0534 - val_acc: 0.9723 - val_loss: 0.1144\n",
      "Epoch 10/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9788 - loss: 0.0845 - val_acc: 0.9908 - val_loss: 0.0658\n",
      "Epoch 11/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - acc: 0.9798 - loss: 0.0660 - val_acc: 0.9963 - val_loss: 0.0471\n",
      "Epoch 12/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - acc: 0.9884 - loss: 0.0440 - val_acc: 0.9963 - val_loss: 0.0442\n",
      "Epoch 13/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.9907 - loss: 0.0422 - val_acc: 0.9963 - val_loss: 0.0420\n",
      "Epoch 14/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.9878 - loss: 0.0310 - val_acc: 0.9945 - val_loss: 0.0386\n",
      "Epoch 15/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.9935 - loss: 0.0243 - val_acc: 0.9963 - val_loss: 0.0508\n",
      "Epoch 16/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.9921 - loss: 0.0244 - val_acc: 0.9908 - val_loss: 0.0494\n",
      "Epoch 17/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.9966 - loss: 0.0164 - val_acc: 0.9926 - val_loss: 0.0403\n",
      "Epoch 18/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - acc: 0.9918 - loss: 0.0253 - val_acc: 0.9963 - val_loss: 0.0448\n",
      "Epoch 19/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.9924 - loss: 0.0401 - val_acc: 0.9926 - val_loss: 0.0390\n",
      "Epoch 20/20\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - acc: 0.9933 - loss: 0.0469 - val_acc: 0.9908 - val_loss: 0.0527\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sign_images, categories, test_size=0.2, random_state=42)\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomContrast([1.0, 10.0]),\n",
    "  layers.RandomTranslation(height_factor=0.02, width_factor=0.02, fill_mode=\"constant\")\n",
    "])\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3,3), activation='relu', # (3,3) - фильтр\n",
    "                        input_shape=(image_size[0],image_size[1],1)),\n",
    "    layers.MaxPooling2D((2,2)), # фильтр (2,2) для пулинга\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, 'relu'),\n",
    "    layers.Dense(len(sign_categories), 'softmax')\n",
    "])\n",
    "   \n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# model.summary()\n",
    "training_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563539d-4781-4e62-a45b-fd9d0dad6c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
